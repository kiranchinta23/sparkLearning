{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "055f1778-6abc-45bf-a810-53a5ca25b549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64a41362-8ef3-42b0-bcfe-943789dd8093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/06/13 11:12:05 WARN Utils: Your hostname, kirans-mac.local, resolves to a loopback address: 127.0.0.1; using 172.18.197.149 instead (on interface en0)\n",
      "25/06/13 11:12:05 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/13 11:12:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "----------------------------------------\n",
      "Exception occurred during processing of request from ('127.0.0.1', 50864)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/lib/python3.11/socketserver.py\", line 317, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/miniconda3/lib/python3.11/socketserver.py\", line 348, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/miniconda3/lib/python3.11/socketserver.py\", line 361, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/miniconda3/lib/python3.11/socketserver.py\", line 755, in __init__\n",
      "    self.handle()\n",
      "  File \"/opt/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 299, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/opt/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 271, in poll\n",
      "    if self.rfile in r and func():\n",
      "                           ^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.11/site-packages/pyspark/accumulators.py\", line 275, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/opt/miniconda3/lib/python3.11/site-packages/pyspark/serializers.py\", line 597, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15948f59-eee7-4d20-9a3d-d646ea576f9f",
   "metadata": {},
   "source": [
    "# create Dataframe from range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d355c686-bd56-4117-b00e-f5bcd5dd5e8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "|  1|\n",
      "|  2|\n",
      "|  3|\n",
      "|  4|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4be44a2-7230-4785-9e53-e147a7e14852",
   "metadata": {},
   "source": [
    "# create single column DataFrame using list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3ad9635-c354-4d62-955e-d1db3952053d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|value|\n",
      "+-----+\n",
      "|   10|\n",
      "|   20|\n",
      "|   30|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# integer datatype values\n",
    "ages_list =[10,20,30]\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "df = spark.createDataFrame(ages_list, IntegerType()) # if type is not included it will fail\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d928ee60-812b-4cbb-8d45-8c121e46bb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "| value|\n",
      "+------+\n",
      "| kiran|\n",
      "| kuamr|\n",
      "|chinta|\n",
      "+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# string datatypes list\n",
    "names = ['kiran','kuamr','chinta']\n",
    "\n",
    "from pyspark.sql.types import *\n",
    "df = spark.createDataFrame(names,StringType()) # if type is not included it will fail\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74983432-825c-431e-99fe-69e9e1d0e9f8",
   "metadata": {},
   "source": [
    "# create DataFrame using list of Tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "118a08e5-417d-4057-ad2b-b1deb3f327fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| _1|    _2| _3|\n",
      "+---+------+---+\n",
      "|  1| kiran| 10|\n",
      "|  2| kumar| 20|\n",
      "|  3|chinta| 30|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_list = [(1,'kiran',10),(2,'kumar',20),(3,'chinta',30)]\n",
    "\n",
    "df3 = spark.createDataFrame(user_list)\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "959b7cdb-0431-4fbc-91ca-405a67603307",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+\n",
      "|user_id|  name|age|\n",
      "+-------+------+---+\n",
      "|      1| kiran| 10|\n",
      "|      2| kumar| 20|\n",
      "|      3|chinta| 30|\n",
      "+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df4 = spark.createDataFrame(user_list,'user_id int, name string, age int')\n",
    "df4.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c75f6fba-fbbf-4b0e-964d-2177651f0559",
   "metadata": {},
   "source": [
    "# create DataFrame using list of Lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e52ebc95-a9f7-4cf2-aa5e-00cf7bd99df9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+\n",
      "| _1|    _2| _3|\n",
      "+---+------+---+\n",
      "|  1| kiran| 10|\n",
      "|  2| kumar| 20|\n",
      "|  3|chinta| 30|\n",
      "+---+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "user_list = [[1,'kiran',10],[2,'kumar',20],[3,'chinta',30]]\n",
    "\n",
    "df5 = spark.createDataFrame(user_list)\n",
    "df5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9030368d-d0f3-412e-8829-e7647175730d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+\n",
      "|user_id|  name|age|\n",
      "+-------+------+---+\n",
      "|      1| kiran| 10|\n",
      "|      2| kumar| 20|\n",
      "|      3|chinta| 30|\n",
      "+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df6 = spark.createDataFrame(user_list,'user_id int, name string, age int')\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51ca2ef-8c4b-4070-bb68-4587f8c3396a",
   "metadata": {},
   "source": [
    "# create DataFrame using list of Dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5555d0d1-6f44-4e00-a51b-df9954714192",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+-------+\n",
      "|age|  name|user_id|\n",
      "+---+------+-------+\n",
      "| 10| kiran|      1|\n",
      "| 20| kumar|      2|\n",
      "| 30|chinta|      3|\n",
      "+---+------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_list = [{'user_id':1, 'name':'kiran','age':10},\n",
    "             {'user_id':2, 'name':'kumar','age':20},\n",
    "             {'user_id':3, 'name':'chinta','age':30}]\n",
    "\n",
    "df7 = spark.createDataFrame(users_list)\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79fa3679-f7fd-47f6-b81c-35e4bddd96b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49732df5-de36-4e43-9dff-1da2ab995626",
   "metadata": {},
   "source": [
    "# create DataFrame using Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fc99ce50-f04f-4187-b9e0-f420b61b294b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+\n",
      "|user_id|  name|age|\n",
      "+-------+------+---+\n",
      "|      1| kiran| 10|\n",
      "|      2| kumar| 20|\n",
      "|      3|chinta| 30|\n",
      "+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8 = spark.createDataFrame(Row(**user) for user in users_list)\n",
    "\n",
    "df8.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede31bf9-80df-4992-8834-62d3dfac8892",
   "metadata": {},
   "source": [
    "# create DataFrame using pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "22bfbba0-c12a-4f45-9bc9-64e4d5108413",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mBrokenPipeError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:527\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43msendall\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mBrokenPipeError\u001b[39m: [Errno 32] Broken pipe",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:530\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    529\u001b[39m     logger.info(\u001b[33m\"\u001b[39m\u001b[33mError while sending or receiving.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m530\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JNetworkError(\n\u001b[32m    531\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mError while sending\u001b[39m\u001b[33m\"\u001b[39m, e, proto.ERROR_ON_SEND)\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mPy4JNetworkError\u001b[39m: Error while sending",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mConnectionRefusedError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      2\u001b[39m users_list = [{\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m1\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mkiran\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m10\u001b[39m},\n\u001b[32m      3\u001b[39m              {\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m2\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mkumar\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m20\u001b[39m},\n\u001b[32m      4\u001b[39m              {\u001b[33m'\u001b[39m\u001b[33muser_id\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m3\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m:\u001b[33m'\u001b[39m\u001b[33mchinta\u001b[39m\u001b[33m'\u001b[39m,\u001b[33m'\u001b[39m\u001b[33mage\u001b[39m\u001b[33m'\u001b[39m:\u001b[32m30\u001b[39m}]\n\u001b[32m      6\u001b[39m pandas_df = pd.DataFrame(users_list)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m df7 = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreateDataFrame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m df7.show()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/pyspark/sql/session.py:1527\u001b[39m, in \u001b[36mSparkSession.createDataFrame\u001b[39m\u001b[34m(self, data, schema, samplingRatio, verifySchema)\u001b[39m\n\u001b[32m   1525\u001b[39m SparkSession._activeSession = \u001b[38;5;28mself\u001b[39m\n\u001b[32m   1526\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1527\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_get_j_spark_session_class\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m)\u001b[49m.setActiveSession(\u001b[38;5;28mself\u001b[39m._jsparkSession)\n\u001b[32m   1528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, DataFrame):\n\u001b[32m   1529\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[32m   1530\u001b[39m         errorClass=\u001b[33m\"\u001b[39m\u001b[33mINVALID_TYPE\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1531\u001b[39m         messageParameters={\u001b[33m\"\u001b[39m\u001b[33marg_name\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33marg_type\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mDataFrame\u001b[39m\u001b[33m\"\u001b[39m},\n\u001b[32m   1532\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/pyspark/sql/session.py:668\u001b[39m, in \u001b[36mSparkSession._get_j_spark_session_class\u001b[39m\u001b[34m(jvm)\u001b[39m\n\u001b[32m    666\u001b[39m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[32m    667\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_j_spark_session_class\u001b[39m(jvm: \u001b[33m\"\u001b[39m\u001b[33mJVMView\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mJavaClass\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m668\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(jvm, \u001b[33m\"\u001b[39m\u001b[33morg.apache.spark.sql.classic.SparkSession\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1752\u001b[39m, in \u001b[36mJVMView.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   1749\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m name == UserHelpAutoCompletion.KEY:\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m UserHelpAutoCompletion()\n\u001b[32m-> \u001b[39m\u001b[32m1752\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_gateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1753\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFLECTION_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1754\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mREFL_GET_UNKNOWN_SUB_COMMAND_NAME\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_id\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\n\u001b[32m   1755\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mproto\u001b[49m\u001b[43m.\u001b[49m\u001b[43mEND_COMMAND_PART\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1756\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m answer == proto.SUCCESS_PACKAGE:\n\u001b[32m   1757\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m JavaPackage(name, \u001b[38;5;28mself\u001b[39m._gateway_client, jvm_id=\u001b[38;5;28mself\u001b[39m._id)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1057\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1055\u001b[39m         retry = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   1056\u001b[39m     logging.info(\u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m, exc_info=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m-> \u001b[39m\u001b[32m1057\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretry\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbinary\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1059\u001b[39m     logging.exception(\n\u001b[32m   1060\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mException while sending command.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/java_gateway.py:1036\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1015\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry=\u001b[38;5;28;01mTrue\u001b[39;00m, binary=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m   1016\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[32m   1017\u001b[39m \u001b[33;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[32m   1018\u001b[39m \u001b[33;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1034\u001b[39m \u001b[33;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[32m   1035\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1036\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1037\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1038\u001b[39m         response = connection.send_command(command)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:284\u001b[39m, in \u001b[36mJavaClient._get_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    281\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m    283\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection.socket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m284\u001b[39m     connection = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:291\u001b[39m, in \u001b[36mJavaClient._create_new_connection\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    288\u001b[39m     connection = ClientServerConnection(\n\u001b[32m    289\u001b[39m         \u001b[38;5;28mself\u001b[39m.java_parameters, \u001b[38;5;28mself\u001b[39m.python_parameters,\n\u001b[32m    290\u001b[39m         \u001b[38;5;28mself\u001b[39m.gateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    292\u001b[39m     \u001b[38;5;28mself\u001b[39m.set_thread_connection(connection)\n\u001b[32m    293\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/lib/python3.11/site-packages/py4j/clientserver.py:438\u001b[39m, in \u001b[36mClientServerConnection.connect_to_java_server\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ssl_context:\n\u001b[32m    436\u001b[39m     \u001b[38;5;28mself\u001b[39m.socket = \u001b[38;5;28mself\u001b[39m.ssl_context.wrap_socket(\n\u001b[32m    437\u001b[39m         \u001b[38;5;28mself\u001b[39m.socket, server_hostname=\u001b[38;5;28mself\u001b[39m.java_address)\n\u001b[32m--> \u001b[39m\u001b[32m438\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    439\u001b[39m \u001b[38;5;28mself\u001b[39m.stream = \u001b[38;5;28mself\u001b[39m.socket.makefile(\u001b[33m\"\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    440\u001b[39m \u001b[38;5;28mself\u001b[39m.is_connected = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mConnectionRefusedError\u001b[39m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "users_list = [{'user_id':1, 'name':'kiran','age':10},\n",
    "             {'user_id':2, 'name':'kumar','age':20},\n",
    "             {'user_id':3, 'name':'chinta','age':30}]\n",
    "\n",
    "pandas_df = pd.DataFrame(users_list)\n",
    "\n",
    "df7 = spark.createDataFrame(pandas_df)\n",
    "df7.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6fddac-e0ac-47b4-8bea-f99add33954e",
   "metadata": {},
   "source": [
    "# describe spark dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b3269168-23ad-4395-a8eb-5975e98df72e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+\n",
      "|user_id|  name|age|\n",
      "+-------+------+---+\n",
      "|      1| kiran| 10|\n",
      "|      2| kumar| 20|\n",
      "|      3|chinta| 30|\n",
      "+-------+------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "68be43d0-27bc-4ddc-9269-689a5b7076e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['user_id', 'name', 'age']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8d8b1f4a-75d1-45dc-823c-1abfecaba565",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('user_id', 'bigint'), ('name', 'string'), ('age', 'bigint')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df8.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ffc5f187-fb99-4859-9acb-1e7684715193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df8.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a05dc4a-6352-4dd4-b2e6-07a9ec0b60b4",
   "metadata": {},
   "source": [
    "# create DataFrame with Schema(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3adb778-3dd3-443b-a5d1-5d43d532fb09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+---+----------+-------------------+---------+-------+\n",
      "|user_id|  name|age|       dob|               slot|available|    sal|\n",
      "+-------+------+---+----------+-------------------+---------+-------+\n",
      "|      1| kiran| 10|1990-10-15|1990-02-13 01:15:00|     true|1200.34|\n",
      "|      2| kumar| 20|1991-11-15|1990-04-01 01:15:00|    false|2200.34|\n",
      "|      3|chinta| 30|1991-12-15|1990-06-05 01:15:00|     true|4200.34|\n",
      "+-------+------+---+----------+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "users_list = [(1,'kiran',10, datetime.date(1990,10,15),datetime.datetime(1990,2,13,1,15),True,1200.34),\n",
    "             (2,'kumar',20, datetime.date(1991,11,15),datetime.datetime(1990,4,1,1,15),False,2200.34),\n",
    "             (3,'chinta',30, datetime.date(1991,12,15),datetime.datetime(1990,6,5,1,15),True,4200.34)]\n",
    "\n",
    "\n",
    "schema = 'user_id INT, name STRING, age INT, dob DATE, slot TIMESTAMP, available BOOLEAN, sal FLOAT'\n",
    "df9 = spark.createDataFrame(users_list, schema = schema)\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6c896832-b37e-432c-ae30-bf9614f9dbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- dob: date (nullable = true)\n",
      " |-- slot: timestamp (nullable = true)\n",
      " |-- available: boolean (nullable = true)\n",
      " |-- sal: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df9.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed3c40c-98be-4f98-b5ec-a6b126537bca",
   "metadata": {},
   "source": [
    "# create DataFrame with Schema(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f224ecd0-0707-4f03-ab38-02f3ae0033fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+-------+----------+-------------------+-----------------+---------+\n",
      "|user_id INT|name STRING|age INT|  dob DATE|     slot TIMESTAMP|available BOOLEAN|sal FLOAT|\n",
      "+-----------+-----------+-------+----------+-------------------+-----------------+---------+\n",
      "|          1|      kiran|     10|1990-10-15|1990-02-13 01:15:00|             true|  1200.34|\n",
      "|          2|      kumar|     20|1991-11-15|1990-04-01 01:15:00|            false|  2200.34|\n",
      "|          3|     chinta|     30|1991-12-15|1990-06-05 01:15:00|             true|  4200.34|\n",
      "+-----------+-----------+-------+----------+-------------------+-----------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "users_list = [(1,'kiran',10, datetime.date(1990,10,15),datetime.datetime(1990,2,13,1,15),True,1200.34),\n",
    "             (2,'kumar',20, datetime.date(1991,11,15),datetime.datetime(1990,4,1,1,15),False,2200.34),\n",
    "             (3,'chinta',30, datetime.date(1991,12,15),datetime.datetime(1990,6,5,1,15),True,4200.34)]\n",
    "\n",
    "\n",
    "schema = ['user_id INT', 'name STRING', 'age INT', 'dob DATE', 'slot TIMESTAMP', 'available BOOLEAN', 'sal FLOAT']\n",
    "df9 = spark.createDataFrame(users_list, schema = schema)\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00671446-d325-4ba7-be93-cda32473ad54",
   "metadata": {},
   "source": [
    "# create DataFrame with Schema(sparkTypes/structTypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c50d7101-eefa-429e-989c-f6a3cc3cc9e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+---+----------+-------------------+---------+-------+\n",
      "| id|  name|age|       dob|               slot|available|    sal|\n",
      "+---+------+---+----------+-------------------+---------+-------+\n",
      "|  1| kiran| 10|1990-10-15|1990-02-13 01:15:00|     true|1200.34|\n",
      "|  2| kumar| 20|1991-11-15|1990-04-01 01:15:00|    false|2200.34|\n",
      "|  3|chinta| 30|1991-12-15|1990-06-05 01:15:00|     true|4200.34|\n",
      "+---+------+---+----------+-------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "users_list = [(1,'kiran',10, datetime.date(1990,10,15),datetime.datetime(1990,2,13,1,15),True,1200.34),\n",
    "             (2,'kumar',20, datetime.date(1991,11,15),datetime.datetime(1990,4,1,1,15),False,2200.34),\n",
    "             (3,'chinta',30, datetime.date(1991,12,15),datetime.datetime(1990,6,5,1,15),True,4200.34)]\n",
    "\n",
    "\n",
    "schema = ['user_id INT', 'name STRING', 'age INT', 'dob DATE', 'slot TIMESTAMP', 'available BOOLEAN', 'sal FLOAT']\n",
    "\n",
    "schema = StructType([\n",
    "            StructField('id',IntegerType()),\n",
    "            StructField('name',StringType()),\n",
    "            StructField('age',IntegerType()),\n",
    "            StructField('dob',DateType()),\n",
    "            StructField('slot',TimestampType()),\n",
    "            StructField('available',BooleanType()),\n",
    "            StructField('sal',FloatType())\n",
    "    \n",
    "])\n",
    "\n",
    "df9 = spark.createDataFrame(users_list, schema = schema)\n",
    "df9.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98e76e5c-2eb3-4a36-b564-457eb755eebd",
   "metadata": {},
   "source": [
    "# special dataypes in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e99f5f2-43d7-443e-bb4b-9a6dfd7cdaac",
   "metadata": {},
   "source": [
    "- python --> spark\n",
    "- list --> array\n",
    "- dict --> map\n",
    "-     struct (record in exchange)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec35da-bb9d-4357-9272-4374bfcce05e",
   "metadata": {},
   "source": [
    "# Create DataFrame with ARRAY datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "016be4a4-8331-4a82-ad2c-b029d8f44b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+-------+\n",
      "|age|   name|phone_numbers|user_id|\n",
      "+---+-------+-------------+-------+\n",
      "| 10|  kiran| [1234, 5434]|      1|\n",
      "| 20|  kumar|   [246, 135]|      2|\n",
      "| 30| chinta|   [456, 789]|      3|\n",
      "| 40|  goats|  [NULL, 789]|      4|\n",
      "| 50| manchi|  [456, NULL]|      5|\n",
      "| 60|unknown| [NULL, NULL]|      6|\n",
      "+---+-------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_list = [{'user_id':1, 'name':'kiran','age':10,'phone_numbers':[1234,5434]},\n",
    "             {'user_id':2, 'name':'kumar','age':20,'phone_numbers':[246,135]},\n",
    "             {'user_id':3, 'name':'chinta','age':30, 'phone_numbers':[456,789]},\n",
    "             {'user_id':4, 'name':'goats','age':40, 'phone_numbers':[None,789]},\n",
    "             {'user_id':5, 'name':'manchi','age':50, 'phone_numbers':[456,None]},\n",
    "             {'user_id':6, 'name':'unknown','age':60, 'phone_numbers':[None,None]},]\n",
    "\n",
    "\n",
    "df10 = spark.createDataFrame(users_list)\n",
    "df10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cce56bba-a89f-47ff-bfde-52d94f857d01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone_numbers: array (nullable = true)\n",
      " |    |-- element: long (containsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df10.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bfdfca30-b1bb-4fad-9979-b27a93b9a905",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+-------+----+\n",
      "|age|   name|phone_numbers|user_id| col|\n",
      "+---+-------+-------------+-------+----+\n",
      "| 10|  kiran| [1234, 5434]|      1|1234|\n",
      "| 10|  kiran| [1234, 5434]|      1|5434|\n",
      "| 20|  kumar|   [246, 135]|      2| 246|\n",
      "| 20|  kumar|   [246, 135]|      2| 135|\n",
      "| 30| chinta|   [456, 789]|      3| 456|\n",
      "| 30| chinta|   [456, 789]|      3| 789|\n",
      "| 40|  goats|  [NULL, 789]|      4|NULL|\n",
      "| 40|  goats|  [NULL, 789]|      4| 789|\n",
      "| 50| manchi|  [456, NULL]|      5| 456|\n",
      "| 50| manchi|  [456, NULL]|      5|NULL|\n",
      "| 60|unknown| [NULL, NULL]|      6|NULL|\n",
      "| 60|unknown| [NULL, NULL]|      6|NULL|\n",
      "+---+-------+-------------+-------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode --> will create seperate columns by flattening the data\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df10.select('*',explode('phone_numbers')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d6876500-375e-4603-a3e3-ec434beefb0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|user_id|home_number|mobile_number|\n",
      "+-------+-----------+-------------+\n",
      "|      1|       1234|         5434|\n",
      "|      2|        246|          135|\n",
      "|      3|        456|          789|\n",
      "|      4|       NULL|          789|\n",
      "|      5|        456|         NULL|\n",
      "|      6|       NULL|         NULL|\n",
      "+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create two seperate columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df10.\\\n",
    "    select('user_id',col('phone_numbers')[0].alias('home_number'),col('phone_numbers')[1].alias('mobile_number')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9acc4f-2e58-4966-b274-9e3203d04deb",
   "metadata": {},
   "source": [
    "# Create DataFrame with MAP datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "19404602-748a-4955-ac7f-119bb93847df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+------------------------------+-------+\n",
      "|age|name   |phone_numbers                 |user_id|\n",
      "+---+-------+------------------------------+-------+\n",
      "|10 |kiran  |{mobile -> 456, home -> 123}  |1      |\n",
      "|20 |kumar  |{mobile -> 680, home -> 246}  |2      |\n",
      "|30 |chinta |{mobile -> 579, home -> 135}  |3      |\n",
      "|40 |goats  |{mobile -> NULL, home -> 111} |4      |\n",
      "|50 |manchi |{mobile -> 222, home -> NULL} |5      |\n",
      "|60 |unknown|{mobile -> NULL, home -> NULL}|6      |\n",
      "+---+-------+------------------------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_list = [{'user_id':1, 'name':'kiran','age':10,'phone_numbers':{\"home\":123,\"mobile\":456}},\n",
    "             {'user_id':2, 'name':'kumar','age':20,'phone_numbers':{\"home\":246,\"mobile\":680}},\n",
    "             {'user_id':3, 'name':'chinta','age':30, 'phone_numbers':{\"home\":135,\"mobile\":579}},\n",
    "             {'user_id':4, 'name':'goats','age':40, 'phone_numbers':{\"home\":111,\"mobile\":None}},\n",
    "             {'user_id':5, 'name':'manchi','age':50, 'phone_numbers':{\"home\":None,\"mobile\":222}},\n",
    "             {'user_id':6, 'name':'unknown','age':60, 'phone_numbers':{\"home\":None,\"mobile\":None}},]\n",
    "\n",
    "df11 = spark.createDataFrame(users_list)\n",
    "df11.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "9f920728-b40d-4740-9948-44349d16de97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone_numbers: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: long (valueContainsNull = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df11.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "acd8977d-1367-4bfe-8949-dc1c47cfa021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+--------------------+-------+------+-----+\n",
      "|age|   name|       phone_numbers|user_id|   key|value|\n",
      "+---+-------+--------------------+-------+------+-----+\n",
      "| 10|  kiran|{mobile -> 456, h...|      1|mobile|  456|\n",
      "| 10|  kiran|{mobile -> 456, h...|      1|  home|  123|\n",
      "| 20|  kumar|{mobile -> 680, h...|      2|mobile|  680|\n",
      "| 20|  kumar|{mobile -> 680, h...|      2|  home|  246|\n",
      "| 30| chinta|{mobile -> 579, h...|      3|mobile|  579|\n",
      "| 30| chinta|{mobile -> 579, h...|      3|  home|  135|\n",
      "| 40|  goats|{mobile -> NULL, ...|      4|mobile| NULL|\n",
      "| 40|  goats|{mobile -> NULL, ...|      4|  home|  111|\n",
      "| 50| manchi|{mobile -> 222, h...|      5|mobile|  222|\n",
      "| 50| manchi|{mobile -> 222, h...|      5|  home| NULL|\n",
      "| 60|unknown|{mobile -> NULL, ...|      6|mobile| NULL|\n",
      "| 60|unknown|{mobile -> NULL, ...|      6|  home| NULL|\n",
      "+---+-------+--------------------+-------+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# explode --> will create seperate columns by flattening the data\n",
    "\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "df11.select('*',explode('phone_numbers')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1eec76ad-30f1-4a74-baff-0b6ef1d78574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------------+\n",
      "|user_id|home_number|mobile_number|\n",
      "+-------+-----------+-------------+\n",
      "|      1|        123|          456|\n",
      "|      2|        246|          680|\n",
      "|      3|        135|          579|\n",
      "|      4|        111|         NULL|\n",
      "|      5|       NULL|          222|\n",
      "|      6|       NULL|         NULL|\n",
      "+-------+-----------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# create two seperate columns\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df11.\\\n",
    "    select('user_id',col('phone_numbers')['home'].alias('home_number'),col('phone_numbers')['mobile'].alias('mobile_number')).\\\n",
    "    show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acc76f-bacd-4c15-9421-865ee2a544a7",
   "metadata": {},
   "source": [
    "# create DataFrame using STRUCT datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "222cb046-b3d9-4e1f-bf31-f764389bffa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+-------------+-------+\n",
      "|age|name   |phone_numbers|user_id|\n",
      "+---+-------+-------------+-------+\n",
      "|10 |kiran  |{123, 456}   |1      |\n",
      "|20 |kumar  |{123, 456}   |2      |\n",
      "|30 |chinta |{123, 456}   |3      |\n",
      "|40 |goats  |{123, 456}   |4      |\n",
      "|50 |manchi |{123, 456}   |5      |\n",
      "|60 |unknown|{123, 456}   |6      |\n",
      "+---+-------+-------------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "users_list = [{'user_id':1, 'name':'kiran','age':10,'phone_numbers':Row(mobile=123,home=456)},\n",
    "             {'user_id':2, 'name':'kumar','age':20,'phone_numbers':Row(mobile=123,home=456)},\n",
    "             {'user_id':3, 'name':'chinta','age':30, 'phone_numbers':Row(mobile=123,home=456)},\n",
    "             {'user_id':4, 'name':'goats','age':40, 'phone_numbers':Row(mobile=123,home=456)},\n",
    "             {'user_id':5, 'name':'manchi','age':50, 'phone_numbers':Row(mobile=123,home=456)},\n",
    "             {'user_id':6, 'name':'unknown','age':60, 'phone_numbers':Row(mobile=123,home=456)}\n",
    "             ]\n",
    "\n",
    "df12 = spark.createDataFrame(users_list)\n",
    "df12.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "bdc723a1-c41e-49ae-80d9-09d2e97ae61e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- age: long (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- phone_numbers: struct (nullable = true)\n",
      " |    |-- mobile: long (nullable = true)\n",
      " |    |-- home: long (nullable = true)\n",
      " |-- user_id: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "91edd4d3-dc79-430e-af8e-0d1667177367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explode --> doesn't work on struct datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "15071e39-0157-4813-9058-4f4704feb597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+----+\n",
      "|user_id|mobile|home|\n",
      "+-------+------+----+\n",
      "|      1|   123| 456|\n",
      "|      2|   123| 456|\n",
      "|      3|   123| 456|\n",
      "|      4|   123| 456|\n",
      "|      5|   123| 456|\n",
      "|      6|   123| 456|\n",
      "+-------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df12.select('user_id','phone_numbers.mobile','phone_numbers.home').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fca01dd-0416-4646-914a-0210069e3935",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
