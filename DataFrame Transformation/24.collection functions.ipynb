{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820f40fe-dce7-4990-a78f-ad5d3267eb1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "89a31b33-e738-4bbf-99e7-c28bda299386",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.4.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b83f47-210a-4f9f-bc80-b2680d0e4be3",
   "metadata": {},
   "source": [
    "# create dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1649d71c-11c5-4891-a051-90928db1d228",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =(('Alicia','Joseph',['Java','Scala','Spark'],{'hair':'black','eye':'brown'}), \\\n",
    "('Robert','Gee',['Spark','Java'],{'hair':'brown','eye':None}), \\\n",
    "('Mike','Bianca',['CSharp',''],{'hair':'red','eye':''}), \\\n",
    "('John','Kumar',None,None), \\\n",
    "('Jeff','L',['1','2'],{}))\n",
    "schema = ('FirstName','LastName','Languages','properties')\n",
    "emp1 = spark.createDataFrame(data=data,schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ada70160-3015-42be-9a3e-85d4d10ea0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=(('Robert',35,40,40),('Ram',31,33,29),('John',95,89,91))\n",
    "schema = ('name','score1','score2','score3')\n",
    "emp2= spark.createDataFrame(data=data, schema=schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fe10d31e-6700-4998-87ee-684a77a1b85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp3 = spark.createDataFrame(data=(\n",
    "    ('John',[10,20,20],[25,11,10]),\\\n",
    "    ('Robert',[15,13,55],[5,None,29]),\\\n",
    "    ('James',[11,13,45],[5,89,79])\\\n",
    "    ),schema=('empName', 'score_arr1', 'score_arr2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b428b33f-ac6e-4dc7-8603-07cfc1787af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+\n",
      "|FirstName|LastName|           Languages|          properties|\n",
      "+---------+--------+--------------------+--------------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|\n",
      "|     John|   Kumar|                NULL|                NULL|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|\n",
      "+---------+--------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393706f6-672b-411c-b7c2-29fbfeeb8598",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+\n",
      "|  name|score1|score2|score3|\n",
      "+------+------+------+------+\n",
      "|Robert|    35|    40|    40|\n",
      "|   Ram|    31|    33|    29|\n",
      "|  John|    95|    89|    91|\n",
      "+------+------+------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp2.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "529de999-9d33-41b6-9208-1e303ad1b490",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+\n",
      "|empName|  score_arr1|   score_arr2|\n",
      "+-------+------------+-------------+\n",
      "|   John|[10, 20, 20]| [25, 11, 10]|\n",
      "| Robert|[15, 13, 55]|[5, NULL, 29]|\n",
      "|  James|[11, 13, 45]|  [5, 89, 79]|\n",
      "+-------+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp3.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81410943-6f5b-4680-814b-5618bd6eb46f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# array functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf19c58-57f5-4749-8b26-7a1b88b9dd6e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### 01. size of array/map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fdb4f4f9-398b-4ada-835c-cad21985f237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+-------------+---------------+\n",
      "|FirstName|LastName|           Languages|          properties|# of langauge|# of properties|\n",
      "+---------+--------+--------------------+--------------------+-------------+---------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|            3|              2|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|            2|              2|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|            2|              2|\n",
      "|     John|   Kumar|                NULL|                NULL|         NULL|           NULL|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|            2|              0|\n",
      "+---------+--------+--------------------+--------------------+-------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import size\n",
    "emp1\\\n",
    "    .withColumn('# of langauge',size('languages'))\\\n",
    "    .withColumn('# of properties',size('properties'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d44357-b9b6-4120-972d-ddaf375910c6",
   "metadata": {},
   "source": [
    "### 02. element_at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f7435d9f-5909-4d1a-91e9-eda606400d83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+-----------+---------------+\n",
      "|FirstName|LastName|           Languages|          properties|langauge[2]|properties[eye]|\n",
      "+---------+--------+--------------------+--------------------+-----------+---------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|      Scala|          brown|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|       Java|           NULL|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|           |               |\n",
      "|     John|   Kumar|                NULL|                NULL|       NULL|           NULL|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|          2|           NULL|\n",
      "+---------+--------+--------------------+--------------------+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import element_at\n",
    "emp1\\\n",
    "    .withColumn('langauge[2]',element_at('languages',2))\\\n",
    "    .withColumn('properties[eye]',element_at('properties','eye'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2e8dc-4ad4-4e27-adf5-cab349f465d8",
   "metadata": {},
   "source": [
    "### 03. Create array / struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ea449528-87ec-4994-a9fd-90a8b9e1a813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+------+----------------+---------------+\n",
      "|  name|score1|score2|score3|struct of scores|array of scores|\n",
      "+------+------+------+------+----------------+---------------+\n",
      "|Robert|    35|    40|    40|    {35, 40, 40}|   [35, 40, 40]|\n",
      "|   Ram|    31|    33|    29|    {31, 33, 29}|   [31, 33, 29]|\n",
      "|  John|    95|    89|    91|    {95, 89, 91}|   [95, 89, 91]|\n",
      "+------+------+------+------+----------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import struct,array\n",
    "emp2\\\n",
    "    .withColumn('struct of scores',struct('score1','score2','score3'))\\\n",
    "    .withColumn('array of scores',array('score1','score2','score3'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d9cc2a-4d2c-498a-8ad2-041c011f0399",
   "metadata": {},
   "source": [
    "### 04. array of max/min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "154a5e6d-9f8f-4c48-94f0-2c075ca08cc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+------------+------------+\n",
      "|empName|  score_arr1|   score_arr2|max of array|min of array|\n",
      "+-------+------------+-------------+------------+------------+\n",
      "|   John|[10, 20, 20]| [25, 11, 10]|          20|          10|\n",
      "| Robert|[15, 13, 55]|[5, NULL, 29]|          55|          13|\n",
      "|  James|[11, 13, 45]|  [5, 89, 79]|          45|          11|\n",
      "+-------+------------+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_max,array_min\n",
    "emp3\\\n",
    "    .withColumn('max of array',array_max('score_arr1'))\\\n",
    "    .withColumn('min of array',array_min('score_arr1'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd27fd7-f190-47bb-a4c4-741c32539290",
   "metadata": {},
   "source": [
    "### 04. array_distinct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "f9c521e2-09b2-402e-9fb7-e27d04fd3c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-----------------+\n",
      "|empName|  score_arr1|   score_arr2|distinct of array|\n",
      "+-------+------------+-------------+-----------------+\n",
      "|   John|[10, 20, 20]| [25, 11, 10]|         [10, 20]|\n",
      "| Robert|[15, 13, 55]|[5, NULL, 29]|     [15, 13, 55]|\n",
      "|  James|[11, 13, 45]|  [5, 89, 79]|     [11, 13, 45]|\n",
      "+-------+------------+-------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_distinct\n",
    "emp3\\\n",
    "    .withColumn('distinct of array',array_distinct('score_arr1'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d9382-5a2e-4034-90ba-012203b3e417",
   "metadata": {},
   "source": [
    "### 04.array_repeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2592f002-c35d-4e68-8b93-d2857f5b54b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+----------------------------------------------------------------------+\n",
      "|empName|score_arr1  |score_arr2   |repeat of array                                                       |\n",
      "+-------+------------+-------------+----------------------------------------------------------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[[10, 20, 20], [10, 20, 20], [10, 20, 20], [10, 20, 20], [10, 20, 20]]|\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[[15, 13, 55], [15, 13, 55], [15, 13, 55], [15, 13, 55], [15, 13, 55]]|\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[[11, 13, 45], [11, 13, 45], [11, 13, 45], [11, 13, 45], [11, 13, 45]]|\n",
      "+-------+------------+-------------+----------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_repeat\n",
    "emp3\\\n",
    "    .withColumn('repeat of array',array_repeat('score_arr1',5))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961c2c9-1386-4fae-8067-34a46f773beb",
   "metadata": {},
   "source": [
    "### 05.slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0831ed25-3ff8-45e5-912f-75fdd6cb15ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+--------------+\n",
      "|empName|score_arr1  |score_arr2   |slice of array|\n",
      "+-------+------------+-------------+--------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[10, 20]      |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[15, 13]      |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[11, 13]      |\n",
      "+-------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import slice\n",
    "emp3\\\n",
    "    .withColumn('slice of array',slice('score_arr1',1,2))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f076f9e5-2ae2-4862-83da-74cbeb2f9929",
   "metadata": {},
   "source": [
    "### 06. array position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3a8a6fd9-6d9f-41e5-a0f0-113eda3024de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+----------------------------+\n",
      "|empName|score_arr1  |score_arr2   |position of element in array|\n",
      "+-------+------------+-------------+----------------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |1                           |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|0                           |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |0                           |\n",
      "+-------+------------+-------------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_position\n",
    "emp3\\\n",
    "    .withColumn('position of element in array',array_position('score_arr1',10))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10155d7d-de3f-429a-801e-eb3552897b3d",
   "metadata": {},
   "source": [
    "### 07.sort_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4f83973-b1a5-4dfd-99d4-719d8110dc6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+------------+------------+\n",
      "|empName|score_arr1  |score_arr2   |asc sort    |desc sort   |\n",
      "+-------+------------+-------------+------------+------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[10, 20, 20]|[20, 20, 10]|\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[13, 15, 55]|[55, 15, 13]|\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[11, 13, 45]|[45, 13, 11]|\n",
      "+-------+------------+-------------+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import sort_array\n",
    "emp3\\\n",
    "    .withColumn('asc sort',sort_array('score_arr1'))\\\n",
    "    .withColumn('desc sort',sort_array('score_arr1',asc=False))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ca1232-d4e4-48da-86ad-4d7581125f77",
   "metadata": {},
   "source": [
    "### 08.array_contains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3fc08012-de0c-478e-a9a2-2f14b338866b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+----------------------+\n",
      "|empName|score_arr1  |score_arr2   |array contains element|\n",
      "+-------+------------+-------------+----------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |false                 |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|false                 |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |true                  |\n",
      "+-------+------------+-------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_contains\n",
    "emp3\\\n",
    "    .withColumn('array contains element',array_contains('score_arr1',11))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d55451a-4400-4a6e-998e-a624762ab415",
   "metadata": {},
   "source": [
    "### 09.array_union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "13871c25-ca18-48ca-9a7e-95de10b0821f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-------------------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays union             |\n",
      "+-------+------------+-------------+-------------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[10, 20, 25, 11]         |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[15, 13, 55, 5, NULL, 29]|\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[11, 13, 45, 5, 89, 79]  |\n",
      "+-------+------------+-------------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_union\n",
    "emp3\\\n",
    "    .withColumn('arrays union',array_union('score_arr1','score_arr2'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c72e8d24-f5a5-4539-83d3-c59afebcb14c",
   "metadata": {},
   "source": [
    "### 10.array_except"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c4f3c069-363c-4a81-9167-13fb736edd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays excpet|\n",
      "+-------+------------+-------------+-------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[20]         |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[15, 13, 55] |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[11, 13, 45] |\n",
      "+-------+------------+-------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_except\n",
    "emp3\\\n",
    "    .withColumn('arrays excpet',array_except('score_arr1','score_arr2'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa51fcb-ee79-496c-bf3d-67f2896971f0",
   "metadata": {},
   "source": [
    "### 11.array_intersect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "0113ec26-0c8a-42c8-b129-10997c7bf7c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+----------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays intersect|\n",
      "+-------+------------+-------------+----------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[10]            |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[]              |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[]              |\n",
      "+-------+------------+-------------+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_intersect\n",
    "emp3\\\n",
    "    .withColumn('arrays intersect',array_intersect('score_arr1','score_arr2'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ea6e3c-1940-4413-a164-9d11ec07c845",
   "metadata": {},
   "source": [
    "### 12.array_join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "25f7a5e2-6dbf-4a33-b49c-879b9675805b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-----------+\n",
      "|empName|score_arr1  |score_arr2   |arrays join|\n",
      "+-------+------------+-------------+-----------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |10+20+20   |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|15+13+55   |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |11+13+45   |\n",
      "+-------+------------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import array_join #-> convert to string same like '_'.join(l1)\n",
    "emp3\\\n",
    "    .withColumn('arrays join',array_join('score_arr1','+'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2137e13-b9c9-4565-831f-57419f5d6b0d",
   "metadata": {},
   "source": [
    "### 13.arrays_zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c9ea1686-35b4-43fc-af93-2d14ee9e88dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+-------------------------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays zip                     |\n",
      "+-------+------------+-------------+-------------------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[{10, 25}, {20, 11}, {20, 10}] |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[{15, 5}, {13, NULL}, {55, 29}]|\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[{11, 5}, {13, 89}, {45, 79}]  |\n",
      "+-------+------------+-------------+-------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_zip\n",
    "emp3\\\n",
    "    .withColumn('arrays zip',arrays_zip('score_arr1','score_arr2'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83918e23-c8bf-40d8-8871-47be67aeeda2",
   "metadata": {},
   "source": [
    "### 14.arrays_overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "bede0d29-d21c-43f8-8a12-31757fb23f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+--------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays overlap|\n",
      "+-------+------------+-------------+--------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |true          |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|NULL          |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |false         |\n",
      "+-------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import arrays_overlap\n",
    "emp3\\\n",
    "    .withColumn('arrays overlap',arrays_overlap('score_arr1','score_arr2'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4722c3a-3d5e-482a-b4d0-dd72008f7e96",
   "metadata": {},
   "source": [
    "### 15. shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "c0d49f5f-fa6f-4590-ae8f-fe84b9c0090c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+--------------+\n",
      "|empName|score_arr1  |score_arr2   |arrays shuffle|\n",
      "+-------+------------+-------------+--------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |[20, 20, 10]  |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|[15, 55, 13]  |\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |[13, 11, 45]  |\n",
      "+-------+------------+-------------+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import shuffle\n",
    "emp3\\\n",
    "    .withColumn('arrays shuffle',shuffle('score_arr1'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7880d7ff-141a-4240-b83a-ce76052ca95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4ad971b9-e7d3-4821-b35d-bae63517df24",
   "metadata": {},
   "source": [
    "# MAP functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645edfc3-f788-45b6-a01b-fb9c1bad0e27",
   "metadata": {},
   "source": [
    "### 01.create map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "79e56e10-ffb1-479a-8ee1-1646834838c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+------------------+\n",
      "|FirstName|LastName|           Languages|          properties|        create map|\n",
      "+---------+--------+--------------------+--------------------+------------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|{Alicia -> Joseph}|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|   {Robert -> Gee}|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|  {Mike -> Bianca}|\n",
      "|     John|   Kumar|                NULL|                NULL|   {John -> Kumar}|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|       {Jeff -> L}|\n",
      "+---------+--------+--------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import create_map\n",
    "emp1\\\n",
    "    .withColumn('create map',create_map('FirstName','LastName'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8000b6b-ba74-4363-b123-ec884b8ba363",
   "metadata": {},
   "source": [
    "### 02. create map from arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3784e475-4840-423d-bff7-382dc0fc0476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+-------------+------------------------------+\n",
      "|empName|score_arr1  |score_arr2   |create map                    |\n",
      "+-------+------------+-------------+------------------------------+\n",
      "|John   |[10, 20, 20]|[25, 11, 10] |{10 -> 10, 20 -> 20}          |\n",
      "|Robert |[15, 13, 55]|[5, NULL, 29]|{15 -> 15, 13 -> 13, 55 -> 55}|\n",
      "|James  |[11, 13, 45]|[5, 89, 79]  |{11 -> 11, 13 -> 13, 45 -> 45}|\n",
      "+-------+------------+-------------+------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.mapKeyDedupPolicy\", \"LAST_WIN\")\n",
    "from pyspark.sql.functions import map_from_arrays,array_distinct\n",
    "emp3\\\n",
    "    .withColumn('create map',map_from_arrays('score_arr1','score_arr1'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b17c28-f2f9-4a67-9b64-60b56b7b5c81",
   "metadata": {},
   "source": [
    "### 03.map keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "ef27b6dc-1b33-4c74-8575-1afbc483d726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+---------------+\n",
      "|FirstName|LastName|           Languages|          properties|get keys of map|\n",
      "+---------+--------+--------------------+--------------------+---------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|    [eye, hair]|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|    [eye, hair]|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|    [eye, hair]|\n",
      "|     John|   Kumar|                NULL|                NULL|           NULL|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|             []|\n",
      "+---------+--------+--------------------+--------------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_keys\n",
    "emp1\\\n",
    "    .withColumn('get keys of map',map_keys('properties'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34642261-bcd7-4adf-a6b0-551b9fb2eae2",
   "metadata": {},
   "source": [
    "### 04.map_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "cdd6b5f5-cbe3-42b0-80d9-5e1ef7e36771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+--------------------+-----------------+\n",
      "|FirstName|LastName|           Languages|          properties|get values of map|\n",
      "+---------+--------+--------------------+--------------------+-----------------+\n",
      "|   Alicia|  Joseph|[Java, Scala, Spark]|{eye -> brown, ha...|   [brown, black]|\n",
      "|   Robert|     Gee|       [Spark, Java]|{eye -> NULL, hai...|    [NULL, brown]|\n",
      "|     Mike|  Bianca|          [CSharp, ]|{eye -> , hair ->...|          [, red]|\n",
      "|     John|   Kumar|                NULL|                NULL|             NULL|\n",
      "|     Jeff|       L|              [1, 2]|                  {}|               []|\n",
      "+---------+--------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_values\n",
    "emp1\\\n",
    "    .withColumn('get values of map',map_values('properties'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4dc5da-a6cf-4a76-9b0b-05acacd4b461",
   "metadata": {},
   "source": [
    "### 05.map_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f697ef90-c41e-4225-958d-023c662f3694",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+-----------------------------+-----------------------------+\n",
      "|FirstName|LastName|Languages           |properties                   |map_concat                   |\n",
      "+---------+--------+--------------------+-----------------------------+-----------------------------+\n",
      "|Alicia   |Joseph  |[Java, Scala, Spark]|{eye -> brown, hair -> black}|{eye -> brown, hair -> black}|\n",
      "|Robert   |Gee     |[Spark, Java]       |{eye -> NULL, hair -> brown} |{eye -> NULL, hair -> brown} |\n",
      "|Mike     |Bianca  |[CSharp, ]          |{eye -> , hair -> red}       |{eye -> , hair -> red}       |\n",
      "|John     |Kumar   |NULL                |NULL                         |NULL                         |\n",
      "|Jeff     |L       |[1, 2]              |{}                           |{}                           |\n",
      "+---------+--------+--------------------+-----------------------------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import map_concat\n",
    "emp1\\\n",
    "    .withColumn('map_concat',map_concat('properties','properties'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "cc0562a9-9320-4d2d-b94d-e1229f1d7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbeaead7-ffe5-4239-b3d6-beb33a99f26f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
