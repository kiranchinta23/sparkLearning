{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37e4dd9d-4a76-4bee-b0af-9449a94e9946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "854116ec-7911-4397-b88d-3d80c855ea04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.spark:spark-avro_2.12:3.4.0\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4916941c-6d46-4def-aae3-61655e11ef40",
   "metadata": {},
   "source": [
    "# create dataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "683da00c-053a-42b4-8458-1f261ab96aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+-------+----------------+-----------+\n",
      "|eid|first_name|last_name|  salary|country|    phone_number|        ssn|\n",
      "+---+----------+---------+--------+-------+----------------+-----------+\n",
      "|  1|    kiran |   chinta|150000.0|    usa| +1 232 343 7889|000 00 0000|\n",
      "|  2|    goats |    machi|100000.0|    ind|+91 111 222 7889|123 45 6789|\n",
      "+---+----------+---------+--------+-------+----------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = ((1,'kiran ','chinta',150000.00,'usa','+1 232 343 7889','000 00 0000'),\n",
    "    (2,'goats ',' machi',100000.00,'ind','+91 111 222 7889','123 45 6789')\n",
    "       )\n",
    "schema=\"\"\"eid INT,first_name STRING,last_name STRING,salary FLOAT,country STRING,phone_number STRING,ssn STRING\"\"\"\n",
    "df = spark.createDataFrame(data=data,schema=schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a14b41-9994-4f25-8406-fde7d75af957",
   "metadata": {},
   "source": [
    "# Date manipulation functions\n",
    "\n",
    "- getting current date & time : <i style=\"color:blue\"> current_date, current_timestamp\n",
    "- date & time arithemetic : <i style=\"color:blue\"> date_add, date_sub, datediff, months_between, add_months, next_day\n",
    "- date & time beginning :  <i style=\"color:blue\"> trunc, date_trunc\n",
    "- date & time formatting : <i style=\"color:blue\"> to_date, to_timestamp, date_format\n",
    "- extract : <i style=\"color:blue\"> year,quater,month, hour, minute, weekofyear, dayofyear, dayofweek\n",
    "- unix time : <i style=\"color:blue\"> unix_timestamp(), from_unixtime()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98980a63-00c0-444d-9962-3f48a975967b",
   "metadata": {},
   "source": [
    "###  get current date & timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5020e20a-3a53-42ad-a1f1-599be5deeb5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+---------+--------+-------+----------------+-----------+------------+\n",
      "|eid|first_name|last_name|  salary|country|    phone_number|        ssn|current_date|\n",
      "+---+----------+---------+--------+-------+----------------+-----------+------------+\n",
      "|  1|    kiran |   chinta|150000.0|    usa| +1 232 343 7889|000 00 0000|  2025-06-15|\n",
      "|  2|    goats |    machi|100000.0|    ind|+91 111 222 7889|123 45 6789|  2025-06-15|\n",
      "+---+----------+---------+--------+-------+----------------+-----------+------------+\n",
      "\n",
      "+---+----------+---------+--------+-------+----------------+-----------+-------------------------+\n",
      "|eid|first_name|last_name|salary  |country|phone_number    |ssn        |current_date             |\n",
      "+---+----------+---------+--------+-------+----------------+-----------+-------------------------+\n",
      "|1  |kiran     |chinta   |150000.0|usa    |+1 232 343 7889 |000 00 0000|2025-06-15 15:31:13.25003|\n",
      "|2  |goats     | machi   |100000.0|ind    |+91 111 222 7889|123 45 6789|2025-06-15 15:31:13.25003|\n",
      "+---+----------+---------+--------+-------+----------------+-----------+-------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_date,current_timestamp\n",
    "\n",
    "df.withColumn('current_date',current_date()).show()\n",
    "df.withColumn('current_date',current_timestamp()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dec851d-e07e-487d-b8f8-f5dd7697ec3c",
   "metadata": {},
   "source": [
    "### arithematic operation on date\n",
    "- adding days to date or timestamp : <i style=\"color:blue\"> date_add,\n",
    "- substraction days from date or timestamp:  <i style=\"color:blue\"> date_sub,\n",
    "- getting num of days b/w two days or timestamps:  <i style=\"color:blue\"> datediff,\n",
    "- getting num of months b/w two dates or timestamps:<i style=\"color:blue\"> months_between,\n",
    "- adding months to date or timestamp : <i style=\"color:blue\">add_months,\n",
    "- next_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0530d234-9b6c-4d6e-91ab-cb1bfef77b03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+\n",
      "|date      |timestamp              |\n",
      "+----------+-----------------------+\n",
      "|2023-08-23|2023-08-23 10:00:00.123|\n",
      "|1993-01-23|1993-01-23 21:30:50.143|\n",
      "|1995-08-11|1995-08-11 08:12:34.003|\n",
      "+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [('2023-08-23','2023-08-23 10:00:00.123'),\n",
    "            ('1993-01-23','1993-01-23 21:30:50.143'),\n",
    "            ('1995-08-11','1995-08-11 08:12:34.003')]\n",
    "\n",
    "d_df = spark.createDataFrame(datetimes,schema = 'date STRING, timestamp STRING')\n",
    "d_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fb09e91-0370-41bf-b024-75c86a63f223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------+----------+----------+----------------+------------------------+\n",
      "|date      |timestamp              |add_date  |sub_date  |add_to_timestamp|substract_from_timestamp|\n",
      "+----------+-----------------------+----------+----------+----------------+------------------------+\n",
      "|2023-08-23|2023-08-23 10:00:00.123|2023-09-02|2023-08-21|2023-08-28      |2023-08-22              |\n",
      "|1993-01-23|1993-01-23 21:30:50.143|1993-02-02|1993-01-21|1993-01-28      |1993-01-22              |\n",
      "|1995-08-11|1995-08-11 08:12:34.003|1995-08-21|1995-08-09|1995-08-16      |1995-08-10              |\n",
      "+----------+-----------------------+----------+----------+----------------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_add, date_sub\n",
    "\n",
    "from pyspark.sql.functions import date_add,date_sub\n",
    "d_df.withColumn('add_date',date_add('date',10))\\\n",
    "   .withColumn('sub_date',date_sub('date',2))\\\n",
    "    .withColumn('add_to_timestamp',date_add('timestamp',5))\\\n",
    "    .withColumn('substract_from_timestamp',date_sub('timestamp',1))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "35b01ee3-3923-4387-9a85-8e122345e021",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------+-------------------+\n",
      "|      date|           timestamp|date_diff|date_diff_timestamp|\n",
      "+----------+--------------------+---------+-------------------+\n",
      "|2023-08-23|2023-08-23 10:00:...|     -662|                662|\n",
      "|1993-01-23|1993-01-23 21:30:...|   -11831|              11831|\n",
      "|1995-08-11|1995-08-11 08:12:...|   -10901|              10901|\n",
      "+----------+--------------------+---------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# datediff\n",
    "\n",
    "from pyspark.sql.functions import datediff\n",
    "d_df.withColumn('date_diff',datediff('date',current_date()))\\\n",
    "    .withColumn('date_diff_timestamp',datediff(current_timestamp(),'timestamp'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c1f7a01a-d919-47bc-aeb4-ed7c460788eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------+-------------------+\n",
      "|      date|           timestamp|months_bw_date|months_bw_timestamp|\n",
      "+----------+--------------------+--------------+-------------------+\n",
      "|2023-08-23|2023-08-23 10:00:...|  -21.74193548|        21.74982378|\n",
      "|1993-01-23|1993-01-23 21:30:...| -388.74193548|       388.73434812|\n",
      "|1995-08-11|1995-08-11 08:12:...| -358.12903226|       358.13932721|\n",
      "+----------+--------------------+--------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# months_between\n",
    "from pyspark.sql.functions import months_between,col\n",
    "d_df.withColumn('months_bw_date',months_between('date',current_date()))\\\n",
    "    .withColumn('months_bw_timestamp',months_between(current_timestamp(),'timestamp'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f61fba71-7a9b-40b5-ac24-88cffe1be4e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+---------------+--------------------+\n",
      "|      date|           timestamp|add_months_date|add_months_timestamp|\n",
      "+----------+--------------------+---------------+--------------------+\n",
      "|2023-08-23|2023-08-23 10:00:...|     2023-10-23|          2023-11-23|\n",
      "|1993-01-23|1993-01-23 21:30:...|     1993-03-23|          1993-04-23|\n",
      "|1995-08-11|1995-08-11 08:12:...|     1995-10-11|          1995-11-11|\n",
      "+----------+--------------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# add_months\n",
    "from pyspark.sql.functions import add_months\n",
    "d_df.withColumn('add_months_date',add_months('date',2))\\\n",
    "    .withColumn('add_months_timestamp',add_months('timestamp',3))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7ef3f7-a050-4989-80f4-b4194eaebe14",
   "metadata": {},
   "source": [
    "### trunc operation on date & timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2577420a-1f32-4b5f-867d-95cb0e899376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+------------+-----------------+------------+-----------------+\n",
      "|      date|           timestamp|mm_date_tunc|mm_timestamp_tunc|yy_date_tunc|yy_timestamp_tunc|\n",
      "+----------+--------------------+------------+-----------------+------------+-----------------+\n",
      "|2023-08-23|2023-08-23 10:00:...|  2023-08-01|       2023-08-01|  2023-01-01|       2023-01-01|\n",
      "|1993-01-23|1993-01-23 21:30:...|  1993-01-01|       1993-01-01|  1993-01-01|       1993-01-01|\n",
      "|1995-08-11|1995-08-11 08:12:...|  1995-08-01|       1995-08-01|  1995-01-01|       1995-01-01|\n",
      "+----------+--------------------+------------+-----------------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# trunc (year or month only) -> starting date of month or year\n",
    "\n",
    "from pyspark.sql.functions import trunc\n",
    "\n",
    "d_df\\\n",
    "    .withColumn('mm_date_tunc',trunc('date','MM'))\\\n",
    "    .withColumn('mm_timestamp_tunc',trunc('timestamp','MM'))\\\n",
    "    .withColumn('yy_date_tunc',trunc('date','yy'))\\\n",
    "    .withColumn('yy_timestamp_tunc',trunc('timestamp','yy'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8c54bfef-1f57-4078-ac1b-535b38e3b9eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-----------------------+-------------------------+------------------------+---------------------+-----------------------+----------------------+\n",
      "|      date|           timestamp|day_starting_using_date|month_starting_using_date|year_starting_using_date|day_starting_using_ts|month_starting_using_ts|year_starting_using_ts|\n",
      "+----------+--------------------+-----------------------+-------------------------+------------------------+---------------------+-----------------------+----------------------+\n",
      "|2023-08-23|2023-08-23 10:00:...|    2023-08-23 00:00:00|      2023-08-01 00:00:00|     2023-01-01 00:00:00|  2023-08-23 00:00:00|    2023-08-01 00:00:00|   2023-01-01 00:00:00|\n",
      "|1993-01-23|1993-01-23 21:30:...|    1993-01-23 00:00:00|      1993-01-01 00:00:00|     1993-01-01 00:00:00|  1993-01-23 00:00:00|    1993-01-01 00:00:00|   1993-01-01 00:00:00|\n",
      "|1995-08-11|1995-08-11 08:12:...|    1995-08-11 00:00:00|      1995-08-01 00:00:00|     1995-01-01 00:00:00|  1995-08-11 00:00:00|    1995-08-01 00:00:00|   1995-01-01 00:00:00|\n",
      "+----------+--------------------+-----------------------+-------------------------+------------------------+---------------------+-----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_trunc (year or month or day ) -> starting timestamp of day month or year\n",
    "\n",
    "from pyspark.sql.functions import date_trunc\n",
    "\n",
    "d_df\\\n",
    "    .withColumn('day_starting_using_date',date_trunc('dd','date'))\\\n",
    "    .withColumn('month_starting_using_date',date_trunc('mm','date'))\\\n",
    "    .withColumn('year_starting_using_date',date_trunc('yy','date'))\\\n",
    "    .withColumn('day_starting_using_ts',date_trunc('dd','timestamp'))\\\n",
    "    .withColumn('month_starting_using_ts',date_trunc('mm','timestamp'))\\\n",
    "    .withColumn('year_starting_using_ts',date_trunc('yy','timestamp'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5f286d35-66ce-4ca6-a305-c74eadc5c97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+--------------------+---------------------+---------------------+----------------------+----------------------+\n",
      "|      date|           timestamp|hh_starting_using_ts|min_starting_using_ts|sec_starting_using_ts|week_starting_using_ts|quat_starting_using_ts|\n",
      "+----------+--------------------+--------------------+---------------------+---------------------+----------------------+----------------------+\n",
      "|2023-08-23|2023-08-23 10:00:...| 2023-08-23 10:00:00|  2023-08-23 10:00:00|  2023-08-23 10:00:00|   2023-08-21 00:00:00|   2023-07-01 00:00:00|\n",
      "|1993-01-23|1993-01-23 21:30:...| 1993-01-23 21:00:00|  1993-01-23 21:30:00|  1993-01-23 21:30:50|   1993-01-18 00:00:00|   1993-01-01 00:00:00|\n",
      "|1995-08-11|1995-08-11 08:12:...| 1995-08-11 08:00:00|  1995-08-11 08:12:00|  1995-08-11 08:12:34|   1995-08-07 00:00:00|   1995-07-01 00:00:00|\n",
      "+----------+--------------------+--------------------+---------------------+---------------------+----------------------+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date_trunc (hour, minute, second / week or quater ) -> starting timestamp of day month or year\n",
    "d_df\\\n",
    "    .withColumn('hh_starting_using_ts',date_trunc('hour','timestamp'))\\\n",
    "    .withColumn('min_starting_using_ts',date_trunc('minute','timestamp'))\\\n",
    "    .withColumn('sec_starting_using_ts',date_trunc('second','timestamp'))\\\n",
    "    .withColumn('week_starting_using_ts',date_trunc('week','timestamp'))\\\n",
    "    .withColumn('quat_starting_using_ts',date_trunc('quarter','timestamp'))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016dc8a1-0ea8-426c-86e4-235816aa4fe8",
   "metadata": {},
   "source": [
    "### formatting\n",
    "- to_date, to_timestamp : converts int or string columns to date,timestamp datatypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8cb8d063-bcaa-4be2-a252-38f40286d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+\n",
      "|date    |time                    |\n",
      "+--------+------------------------+\n",
      "|20231212|28_Feb_2024 10:11:12.234|\n",
      "|20121101|18_Apr_2023 09:30:45.234|\n",
      "|20100101|30_Dec_2010 18:45:55.000|\n",
      "+--------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datetimes = [(20231212,'28_Feb_2024 10:11:12.234'),\n",
    "            (20121101,'18_Apr_2023 09:30:45.234'),\n",
    "            (20100101,'30_Dec_2010 18:45:55.000')]\n",
    "\n",
    "d_df = spark.createDataFrame(datetimes, schema = \"date INT, time STRING\")\n",
    "d_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fd9c7799-e42c-43f9-95c4-f4e28b156d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+-----------+-----------------------+\n",
      "|date    |time                    |date_column|timestamp_column       |\n",
      "+--------+------------------------+-----------+-----------------------+\n",
      "|20231212|28_Feb_2024 10:11:12.234|2023-12-12 |2024-02-28 10:11:12.234|\n",
      "|20121101|18_Apr_2023 09:30:45.234|2012-11-01 |2023-04-18 09:30:45.234|\n",
      "|20100101|30_Dec_2010 18:45:55.000|2010-01-01 |2010-12-30 18:45:55    |\n",
      "+--------+------------------------+-----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import to_date,to_timestamp\n",
    "d_df\\\n",
    "    .withColumn('date_column',to_date('date','yyyyMMdd'))\\\n",
    "    .withColumn('timestamp_column',to_timestamp('time','dd_MMM_yyyy HH:mm:ss.SSS'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "01d67046-9d32-4ce1-bb6f-7ea8448aaa37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+----------+----------+----------+--------+--------+--------+\n",
      "|date    |time                    |year      |month     |day       |hour    |minute  |sec     |\n",
      "+--------+------------------------+----------+----------+----------+--------+--------+--------+\n",
      "|20231212|28_Feb_2024 10:11:12.234|2025-06-15|06-15-2025|15-06-2025|17:00:59|00:59-17|59-17:00|\n",
      "|20121101|18_Apr_2023 09:30:45.234|2025-06-15|06-15-2025|15-06-2025|17:00:59|00:59-17|59-17:00|\n",
      "|20100101|30_Dec_2010 18:45:55.000|2025-06-15|06-15-2025|15-06-2025|17:00:59|00:59-17|59-17:00|\n",
      "+--------+------------------------+----------+----------+----------+--------+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# date-foramt --> print the output in the given format\n",
    "from pyspark.sql.functions import date_format\n",
    "\n",
    "d_df\\\n",
    "    .withColumn('year',date_format(current_date(),'yyyy-MM-dd'))\\\n",
    "    .withColumn('month',date_format(current_date(),'MM-dd-yyy'))\\\n",
    "    .withColumn('day',date_format(current_date(),'dd-MM-yyy'))\\\n",
    "    .withColumn('hour',date_format(current_timestamp(),'HH:mm:ss'))\\\n",
    "    .withColumn('minute',date_format(current_timestamp(),'mm:ss-HH'))\\\n",
    "    .withColumn('sec',date_format(current_timestamp(),'ss-HH:mm'))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ec3fa3-052e-4a7c-9715-4e5823daadc9",
   "metadata": {},
   "source": [
    "# extract the info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "e602efd7-cf0c-4351-8bbc-f26b2d1ef3c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------------------+--------------------------+------------+--------------+-------------+------------+--------------+--------------+----------+---------+---------+\n",
      "|date    |time                    |current_timestamp         |current_year|current_qurter|current_month|current_hour|current_minute|current_second|weekofyear|dayofyear|dayofweek|\n",
      "+--------+------------------------+--------------------------+------------+--------------+-------------+------------+--------------+--------------+----------+---------+---------+\n",
      "|20231212|28_Feb_2024 10:11:12.234|2025-06-15 17:07:38.592857|2025        |2             |6            |17          |7             |38            |24        |166      |1        |\n",
      "|20121101|18_Apr_2023 09:30:45.234|2025-06-15 17:07:38.592857|2025        |2             |6            |17          |7             |38            |24        |166      |1        |\n",
      "|20100101|30_Dec_2010 18:45:55.000|2025-06-15 17:07:38.592857|2025        |2             |6            |17          |7             |38            |24        |166      |1        |\n",
      "+--------+------------------------+--------------------------+------------+--------------+-------------+------------+--------------+--------------+----------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, quarter, month, hour, minute, second\n",
    "from pyspark.sql.functions import weekofyear, dayofyear, dayofweek\n",
    "d_df\\\n",
    "    .withColumn('current_timestamp',current_timestamp())\\\n",
    "    .withColumn('current_year',year(current_timestamp()))\\\n",
    "    .withColumn('current_qurter',quarter(current_timestamp()))\\\n",
    "    .withColumn('current_month',month(current_timestamp()))\\\n",
    "    .withColumn('current_hour',hour(current_timestamp()))\\\n",
    "    .withColumn('current_minute',minute(current_timestamp()))\\\n",
    "    .withColumn('current_second',second(current_timestamp()))\\\n",
    "    .withColumn('weekofyear',weekofyear(current_timestamp()))\\\n",
    "    .withColumn('dayofyear',dayofyear(current_timestamp()))\\\n",
    "    .withColumn('dayofweek',dayofweek(current_timestamp()))\\\n",
    "    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791f4093-481a-45bd-b2aa-995cad5b97d8",
   "metadata": {},
   "source": [
    "# timestamp - epoch/unix\n",
    "- an integer started from jan 1st 1970 midnight UTC\n",
    "- its also know as epoch, for evvery second it increases by 1\n",
    "- to convert from normal to epoch - unix_timestamp()\n",
    "- to convert from epoch to normal - from_unixtime()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d983beeb-8244-4d3c-844b-9dace2be795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+----------+-------------------+\n",
      "|    date|                time|epochs_col|          norma_col|\n",
      "+--------+--------------------+----------+-------------------+\n",
      "|20231212|28_Feb_2024 10:11...|1750026441|2025-06-15 17:23:45|\n",
      "|20121101|18_Apr_2023 09:30...|1750026441|2025-06-15 17:23:45|\n",
      "|20100101|30_Dec_2010 18:45...|1750026441|2025-06-15 17:23:45|\n",
      "+--------+--------------------+----------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import unix_timestamp, from_unixtime, lit\n",
    "\n",
    "d_df\\\n",
    "    .withColumn('epochs_col', unix_timestamp(current_timestamp()))\\\n",
    "    .withColumn('norma_col', from_unixtime(lit(1750026225)))\\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8c6c5708-5c49-4b41-95a6-a28d577f0414",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "032f7da0-9b52-4960-be9f-b059989117e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
